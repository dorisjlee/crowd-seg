@article{IRSHAD2014,
author = {IRSHAD, H. and MONTASER-KOUHSARI, L. and WALTZ, G. and BUCUR, O. and NOWAK, J.A. and DONG, F. and KNOBLAUCH, N.W. and BECK, A.H.},
doi = {10.1142/9789814644730_0029},
file = {:Users/dorislee/Dropbox/Papers/IRSHAD et al.{\_}2014{\_}Crowdsourcing Image Annotation for Nucleus Detection and Segmentation in Computational Pathology Evaluating Experts,.pdf:pdf},
isbn = {978-981-4644-72-3},
journal = {Biocomputing 2015},
pages = {294--305},
title = {{Crowdsourcing Image Annotation for Nucleus Detection and Segmentation in Computational Pathology: Evaluating Experts, Automated Methods, and the Crowd}},
url = {http://www.worldscientific.com/doi/abs/10.1142/9789814644730{\_}0029},
year = {2014}
}
@article{Bearman2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02106v5},
author = {Bearman, Amy and Russakovsky, Olga and Ferrari, Vittorio and Fei-fei, Li},
doi = {10.1007/978-3-319-46478-7},
eprint = {arXiv:1506.02106v5},
file = {:Users/dorislee/Dropbox/Papers/Bearman et al.{\_}2016{\_}What's the Point Semantic Segmentation with Point Supervision.pdf:pdf},
isbn = {9783319464787},
journal = {Eccv'16},
keywords = {data annotation,semantic segmentation,weak supervision},
mendeley-groups = {HCI/AI+HCI/crowd/crowd-seg},
pages = {1--16},
title = {{What's the Point : Semantic Segmentation with Point Supervision}},
year = {2016}
}

@article{Y.Y.Boykov2001,
author = {Y.Y.Boykov and M-P.Jolly},
file = {:Users/dorislee/Dropbox/Papers/Boykov{\_}2001{\_}Interactive Graph Cuts for Optimal Boundary {\&} Region Segmentation of Objects in textup{\{}N{\}}-textup{\{}D{\}} Images.pdf:pdf},
isbn = {0769511430},
journal = {Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on},
number = {July},
pages = {105--112},
title = {{Interactive Graph Cuts for Optimal Boundary {\&} Region Segmentation of Objects in $\backslash$textup{\{}N{\}}-$\backslash$textup{\{}D{\}} Images}},
year = {2001}
}
@article{Li2009,
abstract = {Given an image, we propose a hierarchical generative model that classifies the overall scene, recognizes and segments each object component, as well as annotates the image with a list of tags. To our knowledge, this is the first model that performs all three tasks in one coherent framework. For instance, a scene of a dasiapolo gamepsila consists of several visual objects such as dasiahumanpsila, dasiahorsepsila, dasiagrasspsila, etc. In addition, it can be further annotated with a list of more abstract (e.g. dasiaduskpsila) or visually less salient (e.g. dasiasaddlepsila) tags. Our generative model jointly explains images through a visual model and a textual model. Visually relevant objects are represented by regions and patches, while visually irrelevant textual annotations are influenced directly by the overall scene class. We propose a fully automatic learning framework that is able to learn robust scene models from noisy Web data such as images and user tags from Flickr.com. We demonstrate the effectiveness of our framework by automatically classifying, annotating and segmenting images from eight classes depicting sport scenes. In all three tasks, our model significantly outperforms state-of-the-art algorithms.},
author = {Li, Li Jia and Socher, Richard and Fei-Fei, Li},
doi = {10.1109/CVPRW.2009.5206718},
file = {:Users/dorislee/Dropbox/Papers/Li, Socher{\_}Unknown{\_}Towards Total Scene Understanding Classification , Annotation and Segmentation in an Automatic Framework Scene polo.pdf:pdf},
isbn = {9781424439935},
issn = {1063-6919},
journal = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
pages = {2036--2043},
title = {{Towards total scene understanding: Classification, annotation and segmentation in an automatic framework}},
year = {2009}
}
@article{Kokkinos2008,
author = {Kokkinos, Iasonas and Maragos, Petros},
file = {:Users/dorislee/Dropbox/Papers/Kokkinos, Maragos{\_}2008{\_}Synergy Between Object Recognition and Image Segmentation using the Expectation Maximization Algorithm.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
mendeley-groups = {seg},
number = {20},
pages = {1--16},
title = {{Synergy Between Object Recognition and Image Segmentation using the Expectation Maximization Algorithm}},
url = {http://www.mas.ecp.fr/vision/Personnel/iasonas/pubs/KokkinosMaragos{\_}EM{\_}PAMI09.pdf{\%}5Cnpapers2://publication/uuid/E47A86BD-26D3-464A-8F3B-A556D2462657},
volume = {20},
year = {2008}
}
@article{Lin2012,
abstract = {To ensure quality results from crowdsourced tasks, requesters often aggregate worker responses and use one of a plethora of strategies to infer the correct answer from the set of noisy responses. However, all current models assume prior knowledge of all possible outcomes of the task. While not an unreasonable assumption for tasks that can be posited as multiple-choice questions (e.g. n-ary classification), we observe that many tasks do not naturally fit this paradigm, but instead demand a free-response formulation where the outcome space is of infinite size (e.g. audio transcription). We model such tasks with a novel probabilistic graphical model, and design and implement LazySusan, a decision-theoretic controller that dynamically requests responses as necessary in order to infer answers to these tasks. We also design an EM algorithm to jointly learn the parameters of our model while inferring the correct answers to multiple tasks at a time. Live experiments on Amazon Mechanical Turk demonstrate the superiority of LazySusan at solving SAT Math questions, eliminating 83.2{\%} of the error and achieving greater net utility compared to the state-ofthe- art strategy, majority-voting. We also show in live experiments that our EM algorithm outperforms majority-voting on a visualization task that we design.},
archivePrefix = {arXiv},
arxivId = {arXiv preprint arXiv:1210.4870.},
author = {Lin, Christopher H and Mausam and Weld, Daniel S},
eprint = {arXiv preprint arXiv:1210.4870.},
file = {:Users/dorislee/Dropbox/Papers/Lin, Mausam, Weld{\_}2012{\_}Crowdsourcing control Moving beyond multiple choice.pdf:pdf},
isbn = {9780974903989},
journal = {Uai},
mendeley-groups = {seg},
pages = {491----500},
title = {{Crowdsourcing control : Moving beyond multiple choice}},
year = {2012}
}
@article{Karger2013,
abstract = {Crowdsourcing systems like Amazon's Mechanical Turk have emerged as an e ective large-scale human-powered platform for performing tasks in domains such as image classi cation, data entry, recommendation, and proofreading. Since workers are low-paid (a few cents per task) and tasks performed are monotonous, the answers obtained are noisy and hence unreliable. To obtain reliable estimates, it is essential to utilize appropriate inference algorithms (e.g. Majority voting) coupled with structured redundancy through task assignment. Our goal is to obtain the best possible trade-o between reliability and redundancy. In this paper, we consider a general probabilistic model for noisy observations for crowd-sourcing systems and pose the problem of minimizing the total price (i.e. redundancy) that must be paid to achieve a target overall reliability. Concretely, we show that it is possible to obtain an answer to each task correctly with probability 1  " as long as the redundancy per task is O  (K=q) log(K=")  , where each task can have any of the K distinct answers equally likely, q is the crowd-quality parameter that is de ned through a probabilistic model. Further, e ectively this is the best possible redundancy-accuracy trade-o any system design can achieve. Such a single-parameter crisp characterization of the (order-)optimal trade-o between redundancy and reliability has various useful operational consequences. Further, we analyze the robustness of our approach in the presence of adversarial workers and provide a bound on their in uence on the redundancy-accuracy trade-o . Unlike recent prior work [13, 17, 19], our result applies to non-binary (i.e. K {\textgreater} 2) tasks. In e ect, we utilize algorithms for binary tasks (with inhomogeneous error model unlike that in [13, 17, 19]) as key subroutine to obtain answers for K-ary tasks. Technically, the algorithm is based on low-rank approximation of weighted adjacency matrix for a random regular bipartite graph, weighted according to the answers provided by the workers.},
author = {Karger, David R and Oh, Sewoong and Shah, Devavrat},
doi = {10.1145/2494232.2465761},
file = {:Users/dorislee/Dropbox/Papers/Karger, Oh, Shah{\_}2013{\_}Efficient crowdsourcing for multi-class labeling.pdf:pdf},
isbn = {978-1-4503-1900-3},
issn = {01635999},
journal = {ACM SIGMETRICS Performance Evaluation Review},
keywords = {crowdsourcing,human computation,low-rank matrix,random graphs},
mendeley-groups = {seg},
number = {1},
pages = {81},
title = {{Efficient crowdsourcing for multi-class labeling}},
url = {http://dl.acm.org/citation.cfm?id=2494232.2465761},
volume = {41},
year = {2013}
}
@article{Liu2012,
abstract = {Crowdsourcing has become a popular paradigm for labeling large datasets. How- ever, it has given rise to the computational task of aggregating the crowdsourced labels provided by a collection of unreliable annotators. We approach this prob- lem by transforming it into a standard inference problem in graphical models, and applying approximate variational methods, including belief propagation (BP) and mean field (MF). We show that our BP algorithm generalizes both major- ity voting and a recent algorithm by Karger et al. [1], while our MF method is closely related to a commonly used EM algorithm. In both case, we find that the performance of the algorithms critically depends on the choice of a prior distribu- tion on the workers' reliability; by choosing the prior properly, both BP and MF (and EM) perform surprisingly well on both simulated and real-world datasets, competitive with state-of-the-art algorithms based on more complicated modeling assumptions.},
author = {Liu, Qiang and Peng, Jian and Ihler, Alex},
file = {:Users/dorislee/Dropbox/Papers/Liu, Peng, Ihler{\_}2012{\_}Variational Inference for Crowdsourcing.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Nips},
mendeley-groups = {seg},
pages = {701--709},
title = {{Variational Inference for Crowdsourcing}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2012{\_}0328.pdf{\%}5Cnpapers2://publication/uuid/5BE089DC-9B5E-4494-85ED-5926E1560F82},
year = {2012}
}
@article{Heim2016,
archivePrefix = {arXiv},
arxivId = {1611.08527},
author = {Eccv, Anonymous},
eprint = {1611.08527},
file = {:Users/dorislee/Dropbox/Papers/Eccv{\_}Unknown{\_}Clickstream analysis for crowd-based object segmentation with confidence.pdf:pdf},
author={Eric Heim, Alexander Seitel, Christian Stock, Fabian Isensee, Lena Maier-Hein},
keywords = {clickstream analysis,con-,crowdsourcing,fidence estimation,object segmentation,quality control},
mendeley-groups = {seg},
year={2016},
pages = {1--13},
title = {{Clickstream analysis for crowd-based object segmentation with confidence}}
}
@article{Liu2011,
abstract = {In this paper, we study the salient object detection problem for images. We formulate this problem as a binary labeling task where we separate the salient object from the background. We propose a set of novel features, including multiscale contrast, center-surround histogram, and color spatial distribution, to describe a salient object locally, regionally, and globally. A conditional random field is learned to effectively combine these features for salient object detection. Further, we extend the proposed approach to detect a salient object from sequential images by introducing the dynamic salient features. We collected a large image database containing tens of thousands of carefully labeled images by multiple users and a video segment database, and conducted a set of experiments over them to demonstrate the effectiveness of the proposed approach.},
author = {Liu, Tie and Yuan, Zejian and Sun, Jian and Wang, Jingdong and Zheng, Nanning and Tang, Xiaoou and Shum, Heung-Yeung},
doi = {10.1109/TPAMI.2010.70},
file = {:Users/dorislee/Dropbox/Papers/Liu et al.{\_}2011{\_}Learning to detect a salient object.pdf:pdf},
isbn = {1424411807},
issn = {1939-3539},
journal = {Computer Vision and Pattern Recognition (CVPR)},
mendeley-groups = {seg},
number = {2},
pages = {1--8},
pmid = {21193811},
title = {{Learning to detect a salient object}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5432215{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/21193811},
volume = {33},
year = {2011}
}
@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
doi = {10.1007/978-3-319-10602-1_48},
isbn = {978-3-319-10601-4},
issn = {16113349},
journal = {European Conference on Computer Vision (ECCV)},
mendeley-groups = {seg},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}
@article{Torralba2010,
abstract = {Central to the development of computer vision systems is the collection and use of annotated images spanning our visual world. Annotations may include information about the identity, spatial extent, and viewpoint of the objects present in a depicted scene. Such a database is useful for the training and evaluation of computer vision systems. Motivated by the availability of images on the Internet, we introduced a web-based annotation tool that allows online users to label objects and their spatial extent in images. To date, we have collected over 400 000 annotations that span a variety of different scene and object classes. In this paper, we show the contents of the database, its growth over time, and statistics of its usage. In addition, we explore and survey applications of the database in the areas of computer vision and computer graphics. Particularly, we show how to extract the real-world 3-D coordinates of images in a variety of scenes using only the user-provided object annotations. The output 3-D information is comparable to the quality produced by a laser range scanner. We also characterize the space of the images in the database by analyzing 1) statistics of the co-occurrence of large objects in the images and 2) the spatial layout of the labeled images.},
author = {Torralba, Antonio and Russell, Bryan C. and Yuen, Jenny},
doi = {10.1109/JPROC.2010.2050290},
file = {:Users/dorislee/Dropbox/Papers/Torralba, Russell, Yuen{\_}2010{\_}LabelMe Online image annotation and applications.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {3-D,Image database,Image statistics,Object detection,Object recognition,Online annotation tool,Video annotation},
mendeley-groups = {seg},
number = {8},
pages = {1467--1484},
title = {{LabelMe: Online image annotation and applications}},
volume = {98},
year = {2010}
}
@article{Cabezas2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1505.00145v1},
author = {Cabezas, Ferran and Carlier, Axel and Charvillat, Vincent and Salvador, Amaia and Giro-I-Nieto, Xavier},
doi = {10.1109/ICIP.2015.7351606},
eprint = {arXiv:1505.00145v1},
file = {:Users/dorislee/Dropbox/Papers/Cabezas et al.{\_}2015{\_}Quality control in crowdsourced object segmentation.pdf:pdf},
isbn = {9781479983391},
issn = {15224880},
journal = {Proceedings - International Conference on Image Processing, ICIP},
keywords = {Crowdsourcing,Interactive Segmentation,Object Segmentation,Quality Control,Superpixel},
mendeley-groups = {seg},
pages = {4243--4247},
title = {{Quality control in crowdsourced object segmentation}},
volume = {2015-Decem},
year = {2015}
}
@article{AdrianaKovashka2016,
abstract = {The Path to Path-Traced Movies},
author = {{Adriana Kovashka} and {Olga Russakovsky} and {Li Fei-Fei}},
doi = {10.1561/0600000073},
file = {:Users/dorislee/Dropbox/Papers/Adriana Kovashka, Olga Russakovsky, Li Fei-Fei{\_}2016{\_}Crowdsourcing in Computer Vision.pdf:pdf},
issn = {1572-2740},
journal = {Foundations and Trends{\textregistered} in Computer Graphics and Vision},
keywords = {Computer Graphics,Rendering,Rendering: Forward rendering},
mendeley-groups = {seg},
number = {2},
pages = {103--175},
title = {{Crowdsourcing in Computer Vision}},
url = {http://www.nowpublishers.com/article/Details/CGV-073},
volume = {10},
year = {2016}
}
@article{Raykar2009,
abstract = {We describe a probabilistic approach for supervised learning when we have multiple experts/annotators providing (possibly noisy) labels but no absolute gold standard. The proposed algorithm evaluates the different experts and also gives an estimate of the actual hidden labels. Experimental results indicate that the proposed method is superior to the commonly used majority voting baseline.},
author = {Raykar, Vikas C},
doi = {10.1145/1553374.1553488},
file = {:Users/dorislee/Dropbox/Papers/Raykar{\_}2009{\_}Supervised Learning from Multiple Experts Whom to trust when everyone lies a bit.pdf:pdf},
isbn = {9781605585161},
journal = {New York},
mendeley-groups = {seg},
pages = {1--8},
title = {{Supervised Learning from Multiple Experts : Whom to trust when everyone lies a bit}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553488},
year = {2009}
}
@article{Whitehill2009,
abstract = {Modern machine learning-based approaches to computer vision require very large databases of hand labeled images. Some contemporary vision systems already require on the order of millions of images for training (e.g., Omron face detector 9). New Internet-based services allow for a large number of labelers to collab- orate around the world at very low cost. However, using these services brings interesting theoretical and practical challenges: (1) The labelers may have wide ranging levels of expertise which are unknown a priori, and in some cases may be adversarial; (2) images may vary in their level of difficulty; and (3) multiple labels for the same image must be combined to provide an estimate of the actual label of the image. Probabilistic approaches provide a principled way to approach these problems. In this paper we present a probabilistic model and use it to si- multaneously infer the label of each image, the expertise of each labeler, and the difficulty of each image. On both simulated and real data, we demonstrate that the model outperforms the commonly used Majority Vote heuristic for inferring image labels, and is robust to both noisy and adversarial labelers.},
author = {Whitehill, Jacob and Ruvolo, Paul and Wu, Tingfan and Bergsma, Jacob and Movellan, Javier},
file = {:Users/dorislee/Dropbox/Papers/Whitehill et al.{\_}2009{\_}Whose Vote Should Count More Optimal Integration of Labels from Labelers of Unknown Expertise.pdf:pdf},
isbn = {9781615679119},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {seg},
number = {1},
pages = {1--9},
title = {{Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise}},
volume = {22},
year = {2009}
}
@article{Dawid1979,
author = {Dawid, A. P. and Skene, A. M.},
file = {:Users/dorislee/Dropbox/Papers/Dawid, Skene{\_}1979{\_}Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm.pdf:pdf},
keywords = {em algorithm,latent class model,medical example,observer variation},
mendeley-groups = {seg},
number = {1},
pages = {20--28},
title = {{Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm}},
volume = {28},
year = {1979}
}
@article{Ipeirotis2010,
abstract = {Crowdsourcing services, such as Amazon Mechanical Turk, allow for easy distribution of small tasks to a large number of workers. Unfortunately, since manually verifying the quality of the submitted results is hard, malicious workers often take advantage of the verification difficulty and submit answers of low quality. Currently, most requesters rely on redundancy to identify the correct answers. However, redundancy is not a panacea. Massive redundancy is expensive, increasing significantly the cost of crowdsourced solutions. Therefore, we need techniques that will accurately estimate the quality of the workers, allowing for the rejection and blocking of the low-performing workers and spammers. However, existing techniques cannot separate the true (unrecoverable) error rate from the (recoverable) biases that some workers exhibit. This lack of separation leads to incorrect assessments of a worker's quality. We present algorithms that improve the existing state-of-the-art techniques, enabling the separation of bias and error. Our algorithm generates a scalar score representing the inherent quality of each worker. We illustrate how to incorporate cost-sensitive classification errors in the overall framework and how to seamlessly integrate unsu-pervised and supervised techniques for inferring the quality of the workers. We present experimental results demonstrating the performance of the proposed algorithm under a variety of settings. {\textcopyright} 2010 ACM.},
author = {Ipeirotis, Panagiotis G. and Provost, Foster and Wang, Jing},
doi = {10.1145/1837885.1837906},
file = {:Users/dorislee/Dropbox/Papers/Ipeirotis, Provost, Wang{\_}2010{\_}Quality management on Amazon Mechanical Turk.pdf:pdf},
isbn = {9781450302227},
issn = {145030222X},
journal = {Proceedings of the ACM SIGKDD Workshop on Human Computation - HCOMP '10},
mendeley-groups = {seg},
pages = {64},
pmid = {23835650},
title = {{Quality management on Amazon Mechanical Turk}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-77956245055{\&}partnerID=tZOtx3y1},
year = {2010}
}
@article{MDWWelinder2010,
abstract = {Distributing labeling tasks among hundreds or thousands of annotators is an in- creasingly important method for annotating large datasets. We present a method for estimating the underlying value (eg the class) of each image from (noisy) an- notations provided by multiple annotators. ...},
author = {Welinder, Peter and Branson, Steve and Belongie, Serge and Perona, Pietro},
doi = {10.1.1.231.1538},
file = {:Users/dorislee/Dropbox/Papers/Welinder et al.{\_}2010{\_}The Multidimensional Wisdom of Crowds.pdf:pdf},
isbn = {9781617823800},
journal = {NIPS (Conference on Neural Information Processing Systems)},
mendeley-groups = {seg},
pages = {1--9},
title = {{The Multidimensional Wisdom of Crowds}},
url = {http://www.vision.caltech.edu/visipedia/papers/WelinderEtalNIPS10.pdf},
volume = {6},
year = {2010b}
}
@article{Russakovsky2015,
author = {Russakovsky, Olga and Li, Li-Jia and Fei-Fei, Li},
file = {:Users/dorislee/Dropbox/Papers/Russakovsky, Li, Fei-Fei{\_}2015{\_}Best of Both Worlds Human-Machine Collaboration for Object Annotation.pdf:pdf},
isbn = {9781467369640},
mendeley-groups = {seg},
pages = {2121--2131},
title = {{Best of Both Worlds: Human-Machine Collaboration for Object Annotation}},
year = {2015}
}
@article{Vittayakorn2011,
abstract = {As computer vision datasets grow larger, the community is increasingly relying on crowd-sourced annotations to train and test their algorithms. Since the capability of online annotators is considered varied and unpredictable, many strategies have been proposed to " clean " crowd-sourced annotations. However, these strategies typically require more annotations, rather than using the annotation or image content itself. In this paper we propose and evaluate several strategies for automatically estimating the quality of an object annotation. Finally, we show that we can significantly outperform simple baselines by combining multiple image-based anno-tation assessment strategies.},
author = {Vittayakorn, Sirion and Hays, James},
doi = {10.5244/C.25.109},
file = {:Users/dorislee/Dropbox/Papers/Vittayakorn, Hays{\_}2011{\_}Quality Assessment for Crowdsourced Object Annotations.pdf:pdf},
isbn = {1-901725-43-X},
journal = {Procedings of the British Machine Vision Conference 2011},
mendeley-groups = {seg},
pages = {109.1--109.11},
title = {{Quality Assessment for Crowdsourced Object Annotations}},
url = {http://www.bmva.org/bmvc/2011/proceedings/paper109/index.html},
year = {2011}
}
@article{Sameki2015,
author = {Sameki, Mehrnoosh and Gurari, Danna and Betke, Margrit},
file = {:Users/dorislee/Dropbox/Papers/Sameki, Gurari, Betke{\_}2015{\_}Characterizing Image Segmentation Behavior of the Crowd.pdf:pdf},
mendeley-groups = {seg},
pages = {1--4},
title = {{Characterizing Image Segmentation Behavior of the Crowd}},
year = {2015}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
eprint = {1102.0183},
file = {:Users/dorislee/Dropbox/Papers/Krizhevsky, Sutskever, Hinton{\_}2012{\_}ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
mendeley-groups = {seg},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}

@article{OCWelinder2010,
abstract = {Labeling large datasets has become faster, cheaper, and easier with the advent of crowdsourcing services like Amazon Mechanical Turk. How can one trust the labels obtained from such services? We propose a model of the labeling process which includes label uncertainty, as well a multi-dimensional measure of the annotators' ability. From the model we derive an online algorithm that estimates the most likely value of the labels and the annotator abilities. It finds and prioritizes experts when requesting labels, and actively excludes unreliable annotators. Based on labels already obtained, it dynamically chooses which images will be labeled next, and how many labels to request in order to achieve a desired level of confidence. Our algorithm is general and can handle binary, multi-valued, and continuous annotations (e.g. bounding boxes). Experiments on a dataset containing more than 50,000 labels show that our algorithm reduces the number of labels required, and thus the total cost of labeling, by a large factor while keeping error rates low on a variety of datasets.},
author = {Welinder, Peter and Perona, Pietro},
doi = {10.1109/CVPRW.2010.5543189},
file = {:Users/dorislee/Dropbox/Papers/Welinder, Perona{\_}2010{\_}Online crowdsourcing Rating annotators and obtaining cost-effective labels.pdf:pdf},
isbn = {9781424470297},
issn = {2160-7508},
journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010},
mendeley-groups = {HCI/AI+HCI/crowd/crowd-seg},
pages = {25--32},
title = {{Online crowdsourcing: Rating annotators and obtaining cost-effective labels}},
year = {2010a}
}

@InProceedings{MartinFTM01,
  author = {D. Martin and C. Fowlkes and D. Tal and J. Malik},
  title = {A Database of Human Segmented Natural Images and its
           Application to Evaluating Segmentation Algorithms and
           Measuring Ecological Statistics},
  booktitle = {Proc. 8th Int'l Conf. Computer Vision},
  year = {2001},
  month = {July},
  volume = {2},
  pages = {416--423}
}
@misc{pascal-voc-2012,
	author = "Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.",
	title = "The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2012 {(VOC2012)} {R}esults",
	howpublished = "http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html"}	